{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-10-11T14:46:08.286752Z",
     "start_time": "2024-10-11T14:46:07.089074Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully split the file into 20 parts.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 读取csv文件\n",
    "file_path = 'data/clean_data_actors.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 获取每份的大小\n",
    "num_splits = 20\n",
    "split_size = len(df) // num_splits\n",
    "\n",
    "# 创建输出文件夹\n",
    "output_dir = 'data/splits'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 将数据分成20份并保存\n",
    "for i in range(num_splits):\n",
    "    start_idx = i * split_size\n",
    "    end_idx = (i + 1) * split_size if i != num_splits - 1 else len(df)\n",
    "\n",
    "    split_df = df.iloc[start_idx:end_idx]\n",
    "    output_file = os.path.join(output_dir, f'clean_data_actors_part_{i+1}.csv')\n",
    "\n",
    "    # 保存每一份数据\n",
    "    split_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'Successfully split the file into {num_splits} parts.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义需要合并的文件夹路径\n",
    "folders = ['results', 'results_2', 'results_3', 'results_4', 'results_5']\n",
    "\n",
    "# 用于存储所有数据的列表\n",
    "all_data = []\n",
    "\n",
    "# 遍历所有文件夹\n",
    "for folder in folders:\n",
    "    # 获取该文件夹中的所有CSV文件\n",
    "    folder_path = os.path.join('rabbit/data', folder)\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    # 读取每个CSV文件并添加到all_data列表中\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        all_data.append(df)\n",
    "\n",
    "# 将所有数据合并为一个DataFrame\n",
    "merged_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# 保存合并后的数据\n",
    "output_file = 'rabbit/data/merged_results.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'All CSV files merged into {output_file}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 读取 filtered_clean_data_actors.csv 文件\n",
    "file_path = 'rabbit/data/filtered_clean_data_actors.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# 获取每份的大小\n",
    "num_splits = 10\n",
    "split_size = len(df) // num_splits\n",
    "\n",
    "# 创建输出文件夹\n",
    "output_dir = 'rabbit/data/splits'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 将数据分成10份并保存\n",
    "for i in range(num_splits):\n",
    "    start_idx = i * split_size\n",
    "    end_idx = (i + 1) * split_size if i != num_splits - 1 else len(df)\n",
    "\n",
    "    split_df = df.iloc[start_idx:end_idx]\n",
    "    output_file = os.path.join(output_dir, f'filtered_clean_data_actors_part_{i+1}.csv')\n",
    "\n",
    "    # 保存每一份数据\n",
    "    split_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'Successfully split the file into {num_splits} parts.')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV files merged into data/merged_results.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 定义需要合并的文件夹路径\n",
    "folders = ['results', 'results_2', 'results_3', 'results_4', 'results_5', 'result_3-4', 'result_4-5']\n",
    "\n",
    "# 用于存储所有数据的列表\n",
    "all_data = []\n",
    "\n",
    "# 遍历所有文件夹\n",
    "for folder in folders:\n",
    "    # 获取该文件夹中的所有CSV文件\n",
    "    folder_path = os.path.join(folder)\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "\n",
    "    # 读取每个CSV文件并添加到all_data列表中\n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        all_data.append(df)\n",
    "\n",
    "# 将所有数据合并为一个DataFrame\n",
    "merged_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# 保存合并后的数据\n",
    "output_file = 'data/merged_results.csv'\n",
    "merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f'All CSV files merged into {output_file}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T05:51:14.510469Z",
     "start_time": "2024-10-12T05:51:14.286383Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9341473596326445\n",
      "Precision: 0.2536407766990291\n",
      "Recall: 0.9288888888888889\n",
      "F1 Score: 0.3984747378455672\n",
      "AUC: 0.9315813472979232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pz/nmxfbcq936ndq57fkjbr5w5r0000gn/T/ipykernel_25928/1546541746.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['data_label'] = filtered_df['data_label'].map(label_mapping)\n",
      "/var/folders/pz/nmxfbcq936ndq57fkjbr5w5r0000gn/T/ipykernel_25928/1546541746.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['type'] = filtered_df['type'].map(label_mapping)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# 读取 merged_results.csv 和 merged_output.csv 文件\n",
    "merged_results_path = 'data/merged_results.csv'\n",
    "merged_output_path = 'data/merged_output.csv'\n",
    "\n",
    "merged_results_df = pd.read_csv(merged_results_path)\n",
    "merged_output_df = pd.read_csv(merged_output_path)\n",
    "\n",
    "# 合并两个数据集，基于 merged_results 的 contributor 和 merged_output 的 clean_login 字段\n",
    "merged_df = pd.merge(merged_output_df, merged_results_df, left_on='clean_login', right_on='contributor', how='inner')\n",
    "\n",
    "# 保留 'data_label' 和 'type' 列中值为 'Bot', 'Human', 'Unknown' 的数据\n",
    "valid_labels = ['Bot', 'Human', 'Unknown']\n",
    "filtered_df = merged_df[(merged_df['data_label'].isin(valid_labels)) & (merged_df['type'].isin(valid_labels))]\n",
    "\n",
    "# 映射标签为数值，'Bot' -> 1, 'Human' -> 0，忽略 'Unknown'\n",
    "label_mapping = {'Bot': 1, 'Human': 0}\n",
    "filtered_df['data_label'] = filtered_df['data_label'].map(label_mapping)\n",
    "filtered_df['type'] = filtered_df['type'].map(label_mapping)\n",
    "\n",
    "# 删除 'Unknown' 值\n",
    "filtered_df = filtered_df.dropna(subset=['data_label', 'type'])\n",
    "\n",
    "# 获取预测值和真实值\n",
    "y_true = filtered_df['data_label']\n",
    "y_pred = filtered_df['type']\n",
    "\n",
    "# 计算 Accuracy, Precision, Recall, F1 Score, AUC\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='binary')\n",
    "recall = recall_score(y_true, y_pred, average='binary')\n",
    "f1 = f1_score(y_true, y_pred, average='binary')\n",
    "auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "# 打印结果\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1 Score: {f1}\")\n",
    "print(f\"AUC: {auc}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-12T05:51:19.341225Z",
     "start_time": "2024-10-12T05:51:19.194866Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "graph",
   "language": "python",
   "display_name": "graph"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
